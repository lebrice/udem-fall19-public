
## FABRICE NORMANDIN

## NOTE: if you are viewing this in VSCODE, I'd suggest using "view" --> "toggle word wrap", (Alt + Z) by default.


## Acknowledgements:
I did work on this assignment in close collaboration with Rey, Mostafa, and Niki.


Question 0:
    After understanding the above computed reward, experiment with the constants for each component.
        # default values:
        c1, c2, c3 = 1.0, -10.0, -40.0
        reward = (
            c1 * speed * lp.dot_dir +
            c2 * np.abs(lp.dist) +
            c3 * np.abs(col_penalty) # note that I added the abs, for simplicy.
        )    
    
    - What type of behavior does the above reward function penalize?
        
        This reward penalises getting in close proximity to obstacles (via col_penalty), and being far from the center of the lane (via lp.dist). It encourages the robot to have a great speed in the direction parallel to the lane.

    - Is this good or bad in context of autonomous driving?

        This seems good, but having a collision should be avoided at all costs. Furthermore, it doesn't take into consideration collisions with other vehicles.

    - Name some other issues that can arise with single-objective optimization.

        One issue with single-objective optimization is that it is not context-aware.
        For example, in an emergency, an autonomous car might need to be able to reorder or change its priorities, and having a fixed, single-objective would perhaps not be well adapted to this particular situation. 
        The weight of each component of the objective should be allowed to change depending on the context, or another objective could be used altogether.
        
    - In addition, give three sets of constants and explain qualitatively what types of behavior each penalizes or rewards.
        (note, you may want to use a different action policy than random). Place the answers to the above in reinforcement-learning-answers.txt

        It is unclear to me what mechanism I should implement instead of the current random action. Hence I will leave it as it is.
        Here are three potential sets of constants:

        For sake of simplicity, let us consider a total of 100 'points' we can attribute to each of the loss components. This keeps the total 'loss magnitude' the same for each set of parameters, for fair comparision.
        1- Reckless driver:
            c1 = 50.0
            c2 = -10.0
            c3 = -40.0
            This set of parameters rewards going fast in the right direction more than anything else. It still tries to avoid obstacles, but 
            doesn't really mind being far from the center of the lane.
        
        2- Grandma Driver:
            c1 = 5.0
            c2 = -40.0
            c3 = -55.0
            This set of parameters would value being safe and in the middle of the lane, at the expense of driving fast.
        
        3- Taxi Driver:
            C1 = 25
            c2 = -25
            c3 = -50
            This set of parameters places a relatively equal importance to driving fast and in the center of the lane, all the while trying very hard to avoid obstacles.

1. There are subtle, but important, bugs that have been introduced into the code above. Your job is to find them, and explain them in your reinforcement-learning-answers.txt. You'll want to reread the original DQN and DDPG papers in order to better understand the issue, but by answering the following subquestions (please put the answers to these in the submission for full credit), you'll be on the right track:

    Bugs / modifications:
        - Typos / Simple bugs to get the code to run:
            - self.flat_size = 31968
            - self.lr --> self.relu
            - CriticCNN --> Critic
            - args.save_models --> save_models
            - env_timesteps = 1000 (or some other value)
        
        - the start_timesteps should be lower than max_timesteps, and the env_timesteps should be some value between start_timesteps and max_timesteps. (otherwise, the sampled action is always random)  
        
        - The critic's output should be a scalar value for each (observation, action) pair, rather than an [action-size] vector of values in the [-1, 1] range.
        
        - There is no target network for the Actor or for the critic. (this might be a design decision, rather than a bug?)
        
        - target_Q should be detached

    a) Read some literature on actor-critic methods, including the original actor-critic paper.
        - What is an issue that you see related to non-stationarity?

            The environment we obtain observations from changes over time, and the observations we obtain can exhibit a 'trend' due to this change in the environment. Successive samples are highly correlated, and thus the distribution of these samples changes over time.

        - Define what non-stationarity means in the context of machine learning and how it relates to actor-critic methods.

            (from wikipedia: https://en.wikipedia.org/wiki/Stationary_process)
            A stationary process is a process that generates data whose distribution does not depend on time, and thus the mean, variance and covariance of the data don't change when shifted in time.
            
        - In addition, give some hypotheses on why reinforcement learning is much more difficult (from an optimization perspective) than supervised learning, and how the answer to the previous question and this one are related.

            Citing the DQN paper:
                "RL algorithms [...] must be able to learn from a scalar reward signal that is frequently sparse, noisy and delayed. The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and dargets found in supervised learning."
            Also, in the following lines, the issue of non-stationarity is mentioned:
                "Another issue is that most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states. Furthermore, in RL the data distribution changes as the algorithm leans new behaviours, which can be problematic for deep learning methods that assume a fixed underlying distribution."
            This non-stationarity contributes to RL being more difficult than supervised learning from an optimization perspective. 

    b) Replay Buffer:
        - What role does the replay buffer play in off-policy reinforcement learning?

            The replay buffer is used to reduce the variance in the policy updates, "smooth[ing out] the training distribution over many past behaviours".[DQN]
            It reduces the correlation between consecutive update steps, and avoids oscillations or divergence in the parameters.[DQN]

        - It's most important parameter is max_size - how does changing this value (answer for both increasing and decreasing trends) qualitatively affect the training of the algorithm?

            Increasing:
                By increasing the replay buffer size, the training is smoothed out, as older transitions can sampled as part of an update step. However, using too large a value for this max_size, in relation to the total number of steps (max_steps) can cause problems, as very old transitions could be used throughout a large portion of training, making it difficult for the model to converge appropriately. 

            Decreasing:
                By decreasing the buffer size, we increase the correlation between the update steps. When using too low a value, this can lead to overfitting, and the model might "break". (https://spinningup.openai.com/en/latest/algorithms/ddpg.html)

    c) Challenge Question:
        - Briefly, explain how automatic differentiation works.
            
            By keeping track of operations performed on tensors during the forward pass, ML frameworks like pytorch and Tensorflow are able to deduce the backward pass, since the gradient function of each operation is known, as well as the fact that tensors keep track of their parents (the tensors and operations that created them).
            
            When calling `loss.backward()`, pytorch travels backward through the graph by recursively calculating the partial derivatives of each tensor and moving on to the tensor's parents until a 'leaf' node is reached and it has a `requires_grad` attribute equal to `True`, at which point its `grad` attribute is set to the value of the accumulated partial derivatives. 
            This `grad` attribute stores the gradients of the given parameter with respect to the original 'loss' tensor on which the first `backward()` operation was called.
  
            An optimizer can be created and passed a set of variables to watch (leaf nodes in the backward graph). Whenever the `optimizer.zero_grad()` function is called, the `grad` attribute of each variable to set to 0. When calling the `optimizer.step()` function, the optimization algorithm (for example SGD, ADAM, etc) uses the learning_rate and the value of the `grad` attribute to update the parameter's values.

        - In addition, expand on the difference between a single-element tensor (that requires_grad) and a scalar value as it relates to automatic differentiation;
        - when do we want to backpropogate through a single-element tensor, and when do we not?
            
            As described in the previous question, a single-element tensor that requires_grad can be considered a node within a computational graph. When we backpropagate through it with the 'backward()' function, all the parent tensors in the graph that were responsible for creating it will receive a `grad` attribute and be updated when an optimizer step is performed.    
            However, it is sometimes the case, that a tensor that we created using a network should be considered a 'constant', in which case we do not wish to backpropagate the partial derivatives of the network's parameters with respect to this value. 
            
            For example when we are trying to update the network to make `current_Q` closer to `target_Q`, we should detach the `target_Q` tensor, such that we don't accidentally move these two quantities closer to each other, but rather only move `current_Q` closer to `target_Q`. 
        
        - Take a close look at the code and how losses are being backpropogated. On paper or your favorite drawing software, draw out the actor-critic architecture as described in the code, and label how the actor and critic losses are backpropogated. On your diagram, highlight the particular loss that will cause issues with the above code, and fix it.


2. We discussed a case study of DQN in class. The original authors used quite a few tricks to get this to work.
    Detail some of the following, and explain what problem they solve in training the DQN:
    a) Target Networks

        The target networks are networks whose weights are an exponential-moving-average of the Actor and Critic networks' weights. They are used to stabilize, or "smooth-out" the training by reducing the change in the parameters, and to simulate a sort of "memory" in the network as the "knowledge" obtained through past updates doesn't get overwritten by current updates as much. Therefore, the target networks can be thought of as solutions to the problem of catastrophic forgetting, whereby the network forgets how to deal with a previous problem when learning on a new one.

    b) Annealed Learning Rates

        There is no mention of annealed learning rate in the DQN paper. The value of epsilon, used to sample the action to take at each timestep, is annealed from 1.0 (totally random) to 0.1 over the course of the first 1 million training steps. 

    c) Replay Buffer

        The replay buffer, as previously mentioned, is used to reduce the variance in the policy updates, "smooth[ing out] the training distribution over many past behaviours".[DQN] It reduces the correlation between consecutive update steps, and avoids oscillations or divergence in the parameters.[DQN] In doing so, it helps address the problem of non-stationarity in reinforcement learning by making the distribution more stable.

    d) Random Exploration Period

        Strictly speaking, there is no "random exploration period" in the DQN paper, at least not as it was implemented in the given code. Rather, the epsilon of the epsilon-greedy sampling is annealed from 1.0 to 0.1 over the first 1 million steps, making the first sampled actions almost entirely random. After the first 1 million frames, this epsilon was kept at 0.1, making 10% of the sampled actions random. Also, from the DQN paper:
            "Note that when learning by experience replay, it is necessary to learn off-policy (because our current parameters are different to those used to generate the sample)."

    e) Preprocessing the Image

        The frames are cropped to a square and transformed to grayscale. Then, four such frames are stacked, and this is used as the input to the Critic. Therefore, the Critic receives a condensed, fixed-length history. This enables learning some relatively long-term relationship between states, actions and rewards.


    3. Read about either TD3 or Soft Actor Critic; for your choice, summarize what problems they are addressing with the standard actor-critic formulation, and how they solve them.

        I chose to read the TD3 paper. For sake of simplicity, I will mainly focus on comparing TD3 and DDPG, rather than the "standard actor-critic formulation". 

        The main problem this paper addresses is the accumulation of error in Actor-Critic methods. This problem arises when the noisy signal from a function approximator gives too high a value to a bad state, which in turn causes this bad state to be used to update future states, and thus the error can accumulate over the course of training and "Cause arbitrarily bad states to be estimated as high value, resulting in suboptimal policy updates and divergent behaviour"[TD3]

        They solve this in a few different ways:
        1. They use a variation on Double Q-Learning, which had two actors (A1, A2) and two critics (C1, C2), with A1 is optimized with respect to C1 and A2 with respect to C2.
            In their variant, however, they only used one actor, which reduces computational costs, and the Actor uses the minimum of the estimate values of each critic as its target. This is useful in that it "favors underestimations, which do not tend to be propagated during learning, as actions with low value estimates are avoided by the policy."[TD3]

            In practice, the Critic model used for the regular critic and the target critic contain two identical networks, and produce two independant outputs for each (state, action) pair.

            These lines, where the target_Q value is computed, are quite illustrative of that last point:
            (https://github.com/sfujim/TD3/blob/master/TD3.py#L120)
            ```python
            # Compute the target Q value
			target_Q1, target_Q2 = self.critic_target(next_state, next_action)
			target_Q = torch.min(target_Q1, target_Q2)
			target_Q = reward + not_done * self.discount * target_Q 
            ```
            
            The critic loss is the sum of the MSE(current_Q1, target_Q) and MSE(Current_Q2, target_Q). This optimizes both critics to be closer to the target estimate.
            
        2. Noise is added to the output of the target_actor's output
            - However, noise is clipped to a certain magnitude *before* being added to the action, and the resulting action is then clipped to fit the [-max_action, max_action] range. In their code, the default values have this noise clipped at [-0.5, 0.5] range.
        
        3. The policy (actor and actor_target) updates don't happen at every iteration like in DDPG. Instead, the actor is updated every `policy_freq` iterations (defaults to 2).
            This has the effect of first minimizing the estimation error by the critics before introducing a policy update.
            
            (In a way, I metaphorically see this as giving the critics time to deliberate and come to a sort of agreement before updating the actor and target networks.
            For example, let's suppose that the conceptual evaluations of both critics and of the target_critic with respect to a given action are:
                Q1: "YAY, great move!" (very positive)
                Q2: "no, this sucks, dont do that" (negative)
                target_Q: "this is an OK move" (neutral)
            
            There is a high variance in these estimations, which is problematic, because the output of the critics is used in updating the policy, and that "having a noisy gradient for the policy update [...] is known to reduce learning speed as well as hurt performance in practice. [TD3, Sutton & Barto, 1998].

            Now, an update of Gradient Descent is performed for both critics, but not the target_critic. (Note that in practice another action would be sampled, but I omit that here for sake of simplicity and illustration).
            Now would now get something like:
                Q1: "meh, looks pretty good" (positive)
                Q2: "that's not great" (slightly negative)
                target_Q (unchanged): "this is an OK move" (neutral)

            As the two critic networks are updated to get closer to the target_Q value, their outputs become less polarised, the policy update will 
            Therefore, the update performed on the actor, and on the target networks, 
            
            (Also worth noting: The actor's loss is determined by the negative mean of only the first of the two output of the Critic. This is useful as it preserves the independance between the two "critics", and helps reduce the bias in the model.)



